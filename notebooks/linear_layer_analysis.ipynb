{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpikingJelly Linear 层实验分析\n",
    "\n",
    "实验验证：Linear 层的输入/输出是什么？W 扮演什么角色？Linear + Neuron 组合后行为如何？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from spikingjelly.activation_based import neuron, layer, surrogate\n",
    "\n",
    "matplotlib.rcParams['font.sans-serif'] = ['Noto Sans CJK JP', 'Droid Sans Fallback', 'DejaVu Sans']\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print('环境加载成功')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 单独的 Linear 层：输入/输出类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个 Linear 层\n",
    "linear = layer.Linear(4, 3, bias=False, step_mode='s')\n",
    "\n",
    "# 手动设置权重，方便观察\n",
    "with torch.no_grad():\n",
    "    linear.weight.copy_(torch.tensor([\n",
    "        [1.0, 0.5, -0.3, 0.2],\n",
    "        [0.1, 0.8, 0.4, -0.5],\n",
    "        [-0.2, 0.3, 0.9, 0.1]\n",
    "    ]))\n",
    "\n",
    "print('W (权重矩阵):')\n",
    "print(linear.weight.data)\n",
    "print(f'\\nW 形状: {linear.weight.shape} = [out_features={3}, in_features={4}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试1：实数输入\n",
    "x_real = torch.tensor([0.5, 1.2, -0.3, 0.8])\n",
    "y_real = linear(x_real)\n",
    "\n",
    "print('=== 实数输入 ===')\n",
    "print(f'输入: {x_real.tolist()} (实数)')\n",
    "print(f'输出: {y_real.tolist()} (实数)')\n",
    "print(f'手动验证 W @ x: {(linear.weight.data @ x_real).tolist()}')\n",
    "print(f'输出 == W @ x: {torch.allclose(y_real, linear.weight.data @ x_real)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试2：二值(spike)输入\n",
    "x_spike = torch.tensor([1.0, 0.0, 1.0, 0.0])  # 模拟spike: 神经元0和2发放\n",
    "y_spike = linear(x_spike)\n",
    "\n",
    "print('=== Spike 输入 (0/1) ===')\n",
    "print(f'输入: {x_spike.tolist()} (二值: 神经元0和2发放)')\n",
    "print(f'输出: {y_spike.tolist()} (实数)')\n",
    "print(f'\\n物理含义: 输出 = W的第0列 + W的第2列（发放神经元对应的列之和）')\n",
    "print(f'W[:, 0] = {linear.weight.data[:, 0].tolist()}')\n",
    "print(f'W[:, 2] = {linear.weight.data[:, 2].tolist()}')\n",
    "print(f'W[:, 0] + W[:, 2] = {(linear.weight.data[:, 0] + linear.weight.data[:, 2]).tolist()}')\n",
    "print(f'验证一致: {torch.allclose(y_spike, linear.weight.data[:, 0] + linear.weight.data[:, 2])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试3：不同spike模式 → 不同输出\n",
    "spike_patterns = [\n",
    "    [0, 0, 0, 0],  # 无spike\n",
    "    [1, 0, 0, 0],  # 只有神经元0\n",
    "    [0, 1, 0, 0],  # 只有神经元1\n",
    "    [1, 1, 0, 0],  # 神经元0和1\n",
    "    [1, 1, 1, 1],  # 全部发放\n",
    "]\n",
    "\n",
    "print('=== 不同spike模式 → 不同突触电流 ===')\n",
    "print(f'{\"Spike模式\":20s} | {\"输出(突触电流)\":40s} | 含义')\n",
    "print('=' * 90)\n",
    "for sp in spike_patterns:\n",
    "    x = torch.tensor(sp, dtype=torch.float32)\n",
    "    y = linear(x)\n",
    "    fired = [i for i, s in enumerate(sp) if s == 1]\n",
    "    meaning = f'W的第{fired}列之和' if fired else '零电流'\n",
    "    print(f'{str(sp):20s} | {str([round(v, 3) for v in y.tolist()]):40s} | {meaning}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear + PLIF 组合：完整 SNN 层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建 Linear + PLIF 组合\n",
    "snn_linear = layer.Linear(4, 3, bias=False, step_mode='s')\n",
    "snn_neuron = neuron.ParametricLIFNode(\n",
    "    init_tau=2.0, v_threshold=1.0, v_reset=None,  # v_reset=None → soft reset\n",
    "    surrogate_function=surrogate.Sigmoid(),\n",
    "    step_mode='s'\n",
    ")\n",
    "\n",
    "# 设置相同的权重\n",
    "with torch.no_grad():\n",
    "    snn_linear.weight.copy_(torch.tensor([\n",
    "        [1.0, 0.5, -0.3, 0.2],\n",
    "        [0.1, 0.8, 0.4, -0.5],\n",
    "        [-0.2, 0.3, 0.9, 0.1]\n",
    "    ]))\n",
    "\n",
    "print('=== Linear + PLIF (soft reset) ===')\n",
    "print(f'Linear: 4→3')\n",
    "print(f'PLIF: tau={2.0}, V_th={1.0}, soft reset')\n",
    "print(f'W = {snn_linear.weight.data.tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连续送入spike序列，观察完整流程\n",
    "T = 20\n",
    "torch.manual_seed(42)\n",
    "# 随机spike输入 (4维, 每步有些发放有些不发放)\n",
    "x_seq = (torch.rand(T, 4) > 0.5).float()\n",
    "\n",
    "snn_neuron.reset()\n",
    "\n",
    "currents = []  # Linear输出 = 突触电流\n",
    "voltages = []  # PLIF膜电位\n",
    "spikes = []    # PLIF输出spike\n",
    "\n",
    "for t in range(T):\n",
    "    # Step 1: Linear 把spike转成突触电流\n",
    "    I_t = snn_linear(x_seq[t])\n",
    "    currents.append(I_t.detach().clone())\n",
    "    \n",
    "    # Step 2: PLIF 接收电流，更新膜电位，判定是否发放\n",
    "    s_t = snn_neuron(I_t)\n",
    "    voltages.append(snn_neuron.v.detach().clone())\n",
    "    spikes.append(s_t.detach().clone())\n",
    "\n",
    "currents = torch.stack(currents).numpy()\n",
    "voltages = torch.stack(voltages).numpy()\n",
    "spikes = torch.stack(spikes).numpy()\n",
    "\n",
    "print(f'输入: spike序列 {x_seq.shape} (每步4维, 0/1)')\n",
    "print(f'Linear输出: 突触电流 {currents.shape} (每步3维, 实数)')\n",
    "print(f'PLIF输出: spike序列 {spikes.shape} (每步3维, 0/1)')\n",
    "print(f'\\n输入唯一值: {np.unique(x_seq.numpy())}')\n",
    "print(f'Linear输出值域: [{currents.min():.3f}, {currents.max():.3f}] (实数)')\n",
    "print(f'PLIF输出唯一值: {np.unique(spikes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化完整流程：spike输入 → 突触电流 → 膜电位 → spike输出\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 10), sharex=True)\n",
    "t_arr = np.arange(T)\n",
    "\n",
    "# 1. 输入spike\n",
    "ax = axes[0]\n",
    "for i in range(4):\n",
    "    spike_times = np.where(x_seq[:, i].numpy() > 0.5)[0]\n",
    "    ax.scatter(spike_times, np.full_like(spike_times, i), marker='|', s=100, linewidths=2)\n",
    "ax.set_yticks(range(4))\n",
    "ax.set_yticklabels([f'in_{i}' for i in range(4)])\n",
    "ax.set_title('输入: spike (0/1) — 4维', fontweight='bold', loc='left')\n",
    "\n",
    "# 2. Linear输出 = 突触电流\n",
    "ax = axes[1]\n",
    "for i in range(3):\n",
    "    ax.plot(t_arr, currents[:, i], '-o', markersize=3, label=f'I_{i}')\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n",
    "ax.set_title('Linear输出: 突触电流 I[t] = W @ spike[t] (实数) — W的作用', fontweight='bold', loc='left')\n",
    "ax.legend(loc='upper right', fontsize=9)\n",
    "ax.set_ylabel('电流')\n",
    "\n",
    "# 3. PLIF膜电位\n",
    "ax = axes[2]\n",
    "for i in range(3):\n",
    "    ax.plot(t_arr, voltages[:, i], '-', linewidth=1, label=f'V_{i}')\n",
    "ax.axhline(y=1.0, color='red', linestyle='--', alpha=0.4, label='V_th=1.0')\n",
    "ax.set_title('PLIF膜电位: V[t] = β·V[t-1] + I[t] (实数,内部状态)', fontweight='bold', loc='left')\n",
    "ax.legend(loc='upper right', fontsize=9)\n",
    "ax.set_ylabel('膜电位')\n",
    "\n",
    "# 4. 输出spike\n",
    "ax = axes[3]\n",
    "for i in range(3):\n",
    "    spike_times = np.where(spikes[:, i] > 0.5)[0]\n",
    "    ax.scatter(spike_times, np.full_like(spike_times, i), marker='|', s=100, linewidths=2)\n",
    "ax.set_yticks(range(3))\n",
    "ax.set_yticklabels([f'out_{i}' for i in range(3)])\n",
    "ax.set_title('PLIF输出: spike (0/1) — 3维', fontweight='bold', loc='left')\n",
    "ax.set_xlabel('Time step')\n",
    "\n",
    "fig.suptitle('完整 SNN 层: spike → Linear(W) → 突触电流 → PLIF → spike', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\n总结:')\n",
    "print('  输入: spike (0/1)')\n",
    "print('  Linear(W): spike → 突触电流 (W选择发放神经元对应列求和)')\n",
    "print('  PLIF: 突触电流 → 膜电位积累 → 阈值判定 → spike (0/1)')\n",
    "print('  输出: spike (0/1)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. W 的作用：突触权重如何影响信息传递"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对比不同W对相同输入的影响\n",
    "print('=== W 的物理意义 ===')\n",
    "print()\n",
    "\n",
    "# 相同输入: 神经元0发放\n",
    "x = torch.tensor([1.0, 0.0, 0.0, 0.0])\n",
    "print(f'输入: {x.tolist()} (只有神经元0发放)')\n",
    "print()\n",
    "\n",
    "# W的第0列决定了神经元0的spike对下游3个神经元注入多少电流\n",
    "print(f'W[:, 0] = {snn_linear.weight.data[:, 0].tolist()}')\n",
    "print(f'  → 神经元0发放时:')\n",
    "print(f'    对下游神经元0注入电流: {snn_linear.weight.data[0, 0]:.1f} (兴奋性)')\n",
    "print(f'    对下游神经元1注入电流: {snn_linear.weight.data[1, 0]:.1f} (弱兴奋性)')\n",
    "print(f'    对下游神经元2注入电流: {snn_linear.weight.data[2, 0]:.1f} (抑制性)')\n",
    "print()\n",
    "print('W[j, i] > 0: 突触i→j是兴奋性的 (spike促进下游积累)')\n",
    "print('W[j, i] < 0: 突触i→j是抑制性的 (spike抑制下游积累)')\n",
    "print('W[j, i] ≈ 0: 突触i→j几乎无连接')\n",
    "print()\n",
    "print('W的每一列 = 一个前突触神经元发放时对所有后突触神经元的影响')\n",
    "print('W的每一行 = 一个后突触神经元接收所有前突触神经元的影响')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实数输入 vs spike输入通过Linear的区别\n",
    "print('=== 实数输入 vs Spike输入 通过 Linear ===')\n",
    "print()\n",
    "\n",
    "x_real = torch.tensor([0.73, 0.21, 0.95, 0.44])\n",
    "x_spike = torch.tensor([1.0, 0.0, 1.0, 0.0])\n",
    "\n",
    "y_real = snn_linear(x_real)\n",
    "y_spike = snn_linear(x_spike)\n",
    "\n",
    "print(f'实数输入:  {x_real.tolist()}')\n",
    "print(f'Linear输出: {[round(v, 4) for v in y_real.tolist()]}')\n",
    "print(f'  → W各列的加权和, 权重=输入值 (连续加权)')\n",
    "print()\n",
    "print(f'Spike输入: {x_spike.tolist()}')\n",
    "print(f'Linear输出: {[round(v, 4) for v in y_spike.tolist()]}')\n",
    "print(f'  → W中发放列的直接求和 (选择性求和)')\n",
    "print()\n",
    "print('关键区别:')\n",
    "print('  实数输入: 每个输入维度贡献连续权重, 输出是所有列的加权混合')\n",
    "print('  Spike输入: 只有发放的维度贡献, 输出是被选中列的纯净求和')\n",
    "print('  Spike的稀疏性 → 天然的列选择机制')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 汇总：数据流类型链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('''\n",
    "╔══════════════════════════════════════════════════════════════════════╗\n",
    "║              SpikingJelly SNN 层的数据流                            ║\n",
    "╠══════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                     ║\n",
    "║  [spike 0/1]  ──→  Linear(W)  ──→  [实数: 突触电流]                  ║\n",
    "║                      │                                              ║\n",
    "║                 W @ spike                                           ║\n",
    "║                 = 发放列求和                                         ║\n",
    "║                 = 突触电流注入                                       ║\n",
    "║                                                                     ║\n",
    "║  [实数: 突触电流]  ──→  PLIF  ──→  [spike 0/1]                       ║\n",
    "║                         │                                           ║\n",
    "║                  V[t] = β·V[t-1] + I[t]                             ║\n",
    "║                  if V > V_th: spike, V -= V_th                      ║\n",
    "║                                                                     ║\n",
    "╠══════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                     ║\n",
    "║  W 的角色:                                                          ║\n",
    "║    · W[j,i] = 突触 i→j 的连接强度                                   ║\n",
    "║    · W[j,i] > 0: 兴奋性突触                                         ║\n",
    "║    · W[j,i] < 0: 抑制性突触                                         ║\n",
    "║    · spike输入时: W做列选择 (哪些前突触发放→选哪些列)                  ║\n",
    "║    · 实数输入时: W做加权求和 (标准矩阵乘法)                           ║\n",
    "║                                                                     ║\n",
    "║  完整一层 = Linear(W) + PLIF                                        ║\n",
    "║    输入: spike (0/1) 或 实数                                         ║\n",
    "║    中间: 实数 (突触电流)                                             ║\n",
    "║    输出: spike (0/1)                                                 ║\n",
    "║                                                                     ║\n",
    "╚══════════════════════════════════════════════════════════════════════╝\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
